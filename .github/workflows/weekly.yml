name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"   # poniedziałek 05:00 UTC
  workflow_dispatch: {}

# Żeby dwa uruchomienia nie mieszały cache (np. ręczne + cron)
concurrency:
  group: bip-watcher
  cancel-in-progress: false

permissions:
  contents: write
  issues: write

jobs:
  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest

    # GitHub-hosted runner ma twardy limit ~6h na job; dajemy bufor na zapis/artefakty.
    timeout-minutes: 355

    strategy:
      fail-fast: false
      matrix:
        shard: [0, 1, 2, 3]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ===================== CACHE RESTORE (from repo) =====================
      - name: Unpack cache.zip (if exists)
        run: |
          mkdir -p data
          if [ -f data/cache.zip ]; then
            unzip -o data/cache.zip -d data
          else
            echo "No cache.zip found (first run)."
          fi

      # ===================== RUN SCRIPT (this shard only) =====================
      # continue-on-error: true => nawet jeśli jedna gmina wysypie skan,
      # job spróbuje spakować cache/diag i wrzucić artefakty.
      - name: Run watcher (sharded)
        continue-on-error: true
        env:
          SHARD_TOTAL: "4"
          SHARD_INDEX: "${{ matrix.shard }}"
          # soft-deadline dla kodu (jeśli obsłużysz to w bip_watcher.py)
          RUN_DEADLINE_MIN: "345"
        run: |
          python bip_watcher.py

      # ===================== ENSURE cache.json exists (so merge can always run) =====================
      - name: Ensure cache.json exists
        if: always()
        run: |
          mkdir -p data
          if [ ! -f data/cache.json ]; then
            echo "cache.json missing - creating minimal placeholder to allow merge."
            python - << 'PY'
            import json, os
            os.makedirs("data", exist_ok=True)
            cache = {
              "schema": 10,
              "urls_seen": {},
              "content_seen": {},
              "gmina_seeds": {},
              "page_fprints": {},
              "gmina_frontiers": {},
              "gmina_retry": {},
            }
            with open("data/cache.json", "w", encoding="utf-8") as f:
              json.dump(cache, f, ensure_ascii=False, indent=2)
            PY
          fi

      # ===================== PACK SHARD CACHE =====================
      - name: Pack shard cache
        if: always()
        run: |
          rm -f "data/cache_shard_${{ matrix.shard }}.zip"
          cd data
          zip -9 "cache_shard_${{ matrix.shard }}.zip" cache.json
          cd ..

      # ===================== ARTIFACTS (per shard) =====================
      - name: Upload shard outputs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-output-shard-${{ matrix.shard }}
          if-no-files-found: warn
          path: |
            data/cache_shard_${{ matrix.shard }}.zip
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv
            data/cache.json

  merge:
    name: Merge caches + publish
    runs-on: ubuntu-latest
    needs: [scan]
    if: always()
    timeout-minutes: 60

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Download all shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge cache.json from shards into main data/cache.zip
        run: |
          python - << 'PY'
          import os, json, zipfile

          def load_cache_from_zip(zpath):
            with zipfile.ZipFile(zpath, "r") as z:
              with z.open("cache.json") as f:
                return json.load(f)

          def iso_max(a, b):
            if not a: return b
            if not b: return a
            return a if a >= b else b

          def merge_content_seen(a, b):
            out = dict(a or {})
            for k, vb in (b or {}).items():
              va = out.get(k)
              if not isinstance(vb, dict):
                continue
              if not isinstance(va, dict):
                out[k] = vb
                continue

              outv = dict(va)
              outv["found_at"] = iso_max(va.get("found_at",""), vb.get("found_at",""))
              outv["last_checked"] = iso_max(va.get("last_checked",""), vb.get("last_checked",""))

              if (vb.get("last_checked","") or "") >= (va.get("last_checked","") or ""):
                for fld in ("url","title","gmina","etag","last_modified","att_sig"):
                  if vb.get(fld):
                    outv[fld] = vb.get(fld)

              kwa = set(va.get("keywords") or [])
              kwb = set(vb.get("keywords") or [])
              outv["keywords"] = sorted(kwa | kwb)

              pri = {"ZMIANA": 4, "NOWE": 3, "HIT": 2, "NO_MATCH": 1}
              sa = va.get("status") or ""
              sb = vb.get("status") or ""
              outv["status"] = sa if pri.get(sa, 0) >= pri.get(sb, 0) else sb

              out[k] = outv
            return out

          def merge_dict_latest_ts(a, b, ts_field="ts"):
            out = dict(a or {})
            for k, vb in (b or {}).items():
              va = out.get(k)
              if not isinstance(vb, dict):
                continue
              if not isinstance(va, dict):
                out[k] = vb
                continue
              tsa = va.get(ts_field, "") or ""
              tsb = vb.get(ts_field, "") or ""
              out[k] = va if tsa >= tsb else vb
            return out

          shard_zips = []
          for root, _, files in os.walk("artifacts"):
            for fn in files:
              if fn.startswith("cache_shard_") and fn.endswith(".zip"):
                shard_zips.append(os.path.join(root, fn))
          shard_zips.sort()

          if not shard_zips:
            raise SystemExit("No shard cache zips found in artifacts/")

          caches = [load_cache_from_zip(p) for p in shard_zips]
          merged = caches[0]

          for c in caches[1:]:
            merged["schema"] = max(int(merged.get("schema", 0) or 0), int(c.get("schema", 0) or 0))

            ua = merged.get("urls_seen") or {}
            ub = c.get("urls_seen") or {}
            out_urls = dict(ua)
            for k, ts in ub.items():
              out_urls[k] = iso_max(out_urls.get(k, ""), ts)
            merged["urls_seen"] = out_urls

            merged["content_seen"] = merge_content_seen(merged.get("content_seen") or {}, c.get("content_seen") or {})

            merged["gmina_seeds"] = merge_dict_latest_ts(merged.get("gmina_seeds") or {}, c.get("gmina_seeds") or {}, ts_field="ts")
            merged["page_fprints"] = merge_dict_latest_ts(merged.get("page_fprints") or {}, c.get("page_fprints") or {}, ts_field="ts")

            ga = merged.get("gmina_frontiers") or {}
            gb = c.get("gmina_frontiers") or {}
            out_gf = dict(ga)
            for k, vb in gb.items():
              va = out_gf.get(k)
              if not isinstance(vb, list):
                continue
              if not isinstance(va, list) or len(vb) > len(va):
                out_gf[k] = vb
            merged["gmina_frontiers"] = out_gf

            ra = merged.get("gmina_retry") or {}
            rb = c.get("gmina_retry") or {}
            out_gr = dict(ra)
            for k, vb in rb.items():
              va = out_gr.get(k)
              if not isinstance(vb, list):
                continue
              if not isinstance(va, list):
                out_gr[k] = vb
              else:
                s = set(va)
                for u in vb:
                  if u not in s:
                    va.append(u)
                    s.add(u)
                  if len(va) >= 60000:
                    break
                out_gr[k] = va
            merged["gmina_retry"] = out_gr

          os.makedirs("data", exist_ok=True)
          with open("data/cache.json", "w", encoding="utf-8") as f:
            json.dump(merged, f, ensure_ascii=False, indent=2)

          zpath = "data/cache.zip"
          if os.path.exists(zpath):
            os.remove(zpath)
          with zipfile.ZipFile(zpath, "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
            z.write("data/cache.json", arcname="cache.json")

          print(f"Merged {len(shard_zips)} shard caches -> {zpath}")
          PY

      - name: Commit merged cache.zip back to repo
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add data/cache.zip data/cache.json || true
          git status --porcelain

          git commit -m "Merge shard caches [skip ci]" || echo "Nothing to commit"

          # Uodpornienie na równoległe push'e / zmiany w repo
          git pull --rebase || true
          git push || true
