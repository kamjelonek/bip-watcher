name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"   # poniedziałek 05:00 UTC
  workflow_dispatch: {}

permissions:
  contents: write
  issues: write

jobs:
  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    timeout-minutes: 350

    strategy:
      fail-fast: false
      matrix:
        shard: [0, 1, 2, 3]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ===================== CACHE RESTORE (from repo) =====================
      - name: Unpack cache.zip (if exists)
        run: |
          mkdir -p data
          if [ -f data/cache.zip ]; then
            unzip -o data/cache.zip -d data
          else
            echo "No cache.zip found (first run)."
          fi

      # ===================== RUN SCRIPT (this shard only) =====================
      - name: Run watcher (sharded)
        env:
          SHARD_TOTAL: "4"
          SHARD_INDEX: "${{ matrix.shard }}"
        run: |
          python bip_watcher.py

      # ===================== PACK SHARD CACHE =====================
      - name: Pack shard cache
        run: |
          if [ -f data/cache.json ]; then
            rm -f "data/cache_shard_${{ matrix.shard }}.zip"
            cd data
            zip -9 "cache_shard_${{ matrix.shard }}.zip" cache.json
            cd ..
          else
            echo "No cache.json produced!"
            exit 1
          fi

      # ===================== ARTIFACTS (per shard) =====================
      - name: Upload shard outputs
        uses: actions/upload-artifact@v4
        with:
          name: bip-output-shard-${{ matrix.shard }}
          path: |
            data/cache_shard_${{ matrix.shard }}.zip
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv

  merge:
    name: Merge caches + publish
    runs-on: ubuntu-latest
    needs: [scan]
    timeout-minutes: 60

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Download all shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge cache.json from 4 shards into main data/cache.zip
        run: |
          python - << 'PY'
          import os, json, zipfile
          from datetime import datetime

          def load_cache_from_zip(zpath):
            with zipfile.ZipFile(zpath, "r") as z:
              with z.open("cache.json") as f:
                return json.load(f)

          def iso_max(a, b):
            # wybierz nowszy ISO timestamp (lub b, jeśli a puste)
            if not a: return b
            if not b: return a
            return a if a >= b else b

          def merge_content_seen(a, b):
            # klucz = url_dedup sha1, wartości dict
            out = dict(a or {})
            for k, vb in (b or {}).items():
              va = out.get(k)
              if not isinstance(vb, dict):
                continue
              if not isinstance(va, dict):
                out[k] = vb
                continue

              # scalanie pól + priorytety
              outv = dict(va)

              # timestamps
              outv["found_at"] = iso_max(va.get("found_at",""), vb.get("found_at",""))
              outv["last_checked"] = iso_max(va.get("last_checked",""), vb.get("last_checked",""))

              # preferuj nowszy url/title (po last_checked)
              if (vb.get("last_checked","") or "") >= (va.get("last_checked","") or ""):
                for fld in ("url","title","gmina","etag","last_modified","att_sig"):
                  if vb.get(fld):
                    outv[fld] = vb.get(fld)

              # keywords = union
              kwa = set(va.get("keywords") or [])
              kwb = set(vb.get("keywords") or [])
              outv["keywords"] = sorted(kwa | kwb)

              # status: priorytet ZMIANA > NOWE > HIT > NO_MATCH > SEEN/inna
              pri = {"ZMIANA": 4, "NOWE": 3, "HIT": 2, "NO_MATCH": 1}
              sa = va.get("status") or ""
              sb = vb.get("status") or ""
              outv["status"] = sa if pri.get(sa, 0) >= pri.get(sb, 0) else sb

              out[k] = outv
            return out

          def merge_dict_latest_ts(a, b, ts_field="ts"):
            out = dict(a or {})
            for k, vb in (b or {}).items():
              va = out.get(k)
              if not isinstance(vb, dict):
                continue
              if not isinstance(va, dict):
                out[k] = vb
                continue
              tsa = va.get(ts_field, "") or ""
              tsb = vb.get(ts_field, "") or ""
              out[k] = va if tsa >= tsb else vb
            return out

          # znajdź wszystkie shard zipy
          shard_zips = []
          for root, _, files in os.walk("artifacts"):
            for fn in files:
              if fn.startswith("cache_shard_") and fn.endswith(".zip"):
                shard_zips.append(os.path.join(root, fn))
          shard_zips.sort()

          if len(shard_zips) == 0:
            raise SystemExit("No shard cache zips found in artifacts/")

          caches = [load_cache_from_zip(p) for p in shard_zips]

          # startujemy od pierwszego jako baza
          merged = caches[0]

          for c in caches[1:]:
            merged["schema"] = max(int(merged.get("schema", 0) or 0), int(c.get("schema", 0) or 0))

            # urls_seen: union (timestamp max)
            ua = merged.get("urls_seen") or {}
            ub = c.get("urls_seen") or {}
            out_urls = dict(ua)
            for k, ts in ub.items():
              out_urls[k] = iso_max(out_urls.get(k, ""), ts)
            merged["urls_seen"] = out_urls

            # content_seen: merge per rekord
            merged["content_seen"] = merge_content_seen(merged.get("content_seen") or {}, c.get("content_seen") or {})

            # gmina_seeds / page_fprints: wybierz nowsze po ts
            merged["gmina_seeds"] = merge_dict_latest_ts(merged.get("gmina_seeds") or {}, c.get("gmina_seeds") or {}, ts_field="ts")
            merged["page_fprints"] = merge_dict_latest_ts(merged.get("page_fprints") or {}, c.get("page_fprints") or {}, ts_field="ts")

            # frontier/retry: bierz większe (żeby nie zgubić kolejki)
            ga = merged.get("gmina_frontiers") or {}
            gb = c.get("gmina_frontiers") or {}
            out_gf = dict(ga)
            for k, vb in gb.items():
              va = out_gf.get(k)
              if not isinstance(vb, list):
                continue
              if not isinstance(va, list) or len(vb) > len(va):
                out_gf[k] = vb
            merged["gmina_frontiers"] = out_gf

            ra = merged.get("gmina_retry") or {}
            rb = c.get("gmina_retry") or {}
            out_gr = dict(ra)
            for k, vb in rb.items():
              va = out_gr.get(k)
              if not isinstance(vb, list):
                continue
              if not isinstance(va, list):
                out_gr[k] = vb
              else:
                # union (bez duplikatów) z limitem
                s = set(va)
                for u in vb:
                  if u not in s:
                    va.append(u)
                    s.add(u)
                  if len(va) >= 60000:
                    break
                out_gr[k] = va
            merged["gmina_retry"] = out_gr

          os.makedirs("data", exist_ok=True)
          with open("data/cache.json", "w", encoding="utf-8") as f:
            json.dump(merged, f, ensure_ascii=False, indent=2)

          # zip główny cache.zip
          zpath = "data/cache.zip"
          if os.path.exists(zpath):
            os.remove(zpath)
          with zipfile.ZipFile(zpath, "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
            z.write("data/cache.json", arcname="cache.json")

          print(f"Merged {len(shard_zips)} shard caches -> {zpath}")
          PY

      - name: Commit merged cache.zip back to repo
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add data/cache.zip || true
          git commit -m "Merge shard caches [skip ci]" || echo "Nothing to commit"
          git push
