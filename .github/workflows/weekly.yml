name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"
  workflow_dispatch: {}

concurrency:
  group: bip-watcher
  cancel-in-progress: false

permissions:
  contents: write
  issues: write

jobs:
  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    continue-on-error: true
    timeout-minutes: 355

    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        shard: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ========== POBIERANIE CACHE Z POPRZEDNIEGO MERGE ==========
      - name: Download master cache (if exists)
        uses: actions/download-artifact@v4
        with:
          name: bip-master-cache
          path: data
        continue-on-error: true

      - name: Unpack master cache if downloaded
        run: |
          if [ -f data/cache.zip ]; then
            unzip -o data/cache.zip -d data
            echo "✅ Master cache loaded"
          else
            echo "ℹ️ No master cache – starting fresh"
          fi

      # ========== RUN ==========
      - name: Run watcher (sharded)
        continue-on-error: true
        env:
          PYTHONUNBUFFERED: "1"
          SHARD_TOTAL: "58"
          SHARD_INDEX: "${{ matrix.shard }}"
          RUN_DEADLINE_MIN: "345"
          CONCURRENT_GMINY: "20"
          CONCURRENT_REQUESTS: "80"
          LIMIT_PER_HOST: "8"
          RATE_MIN_DELAY: "0.1"
          CHECKPOINT_EVERY_SEC: "900"
        run: |
          python bip_watcher.py

      # ========== WYSYŁKA SHARDOWEGO CACHE ==========
      - name: Upload shard cache
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-shard-${{ matrix.shard }}
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache_shard_${{ matrix.shard }}.json
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv

  merge:
    name: Merge caches + publish report
    runs-on: ubuntu-latest
    needs: [scan]
    if: always()
    timeout-minutes: 60

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # ========== POBIERANIE WSZYSTKICH SHARDOWYCH ARTEFAKTÓW ==========
      - name: Download all shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      # ========== SCALANIE CACHE ==========
      - name: Merge shard caches
        run: |
          python - <<'PY'
          import os, json, glob, zipfile

          def iso_max(a, b):
              return a if (a or "") >= (b or "") else b

          merged = {
              "schema": 10,
              "urls_seen": {},
              "content_seen": {},
              "gmina_seeds": {},
              "page_fprints": {},
              "gmina_frontiers": {},
              "gmina_retry": {},
              "dead_urls": {}
          }

          # Znajdź wszystkie pliki shardowe
          shard_files = glob.glob("artifacts/**/cache_shard_*.json", recursive=True)
          if not shard_files:
              print("No shard cache files found")
          else:
              for f in shard_files:
                  try:
                      with open(f, "r") as fp:
                          data = json.load(fp)
                  except Exception as e:
                      print(f"Error reading {f}: {e}")
                      continue

                  for k in merged:
                      if k == "schema": continue
                      if k not in data: continue
                      if isinstance(merged[k], dict) and isinstance(data[k], dict):
                          if k == "urls_seen":
                              for url, ts in data[k].items():
                                  if url not in merged[k] or iso_max(merged[k].get(url, ""), ts) == ts:
                                      merged[k][url] = ts
                          elif k == "dead_urls":
                              for gkey, urls in data[k].items():
                                  if gkey not in merged[k]:
                                      merged[k][gkey] = []
                                  existing = set(merged[k][gkey])
                                  for u in urls:
                                      if u not in existing:
                                          merged[k][gkey].append(u)
                          else:
                              merged[k].update(data[k])
                      else:
                          merged[k] = data[k]

              print(f"Merged {len(shard_files)} shard files")

          # Zapisz scalony cache.json
          os.makedirs("data", exist_ok=True)
          with open("data/cache.json", "w") as f:
              json.dump(merged, f, indent=2)

          # Spakuj do cache.zip
          with zipfile.ZipFile("data/cache.zip", "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
              z.write("data/cache.json", arcname="cache.json")

          print("OK: data/cache.json and data/cache.zip created")
          PY

      # ========== WYSYŁKA MASTER CACHE ==========
      - name: Upload master cache
        uses: actions/upload-artifact@v4
        with:
          name: bip-master-cache
          retention-days: 30
          path: |
            data/cache.json
            data/cache.zip

      - name: Upload merged report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-merged-report
          retention-days: 30
          path: |
            data/summary_report.txt
            data/log.csv
