name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"
  workflow_dispatch: {}

concurrency:
  group: bip-watcher
  cancel-in-progress: false

permissions:
  contents: write
  issues: write

jobs:
  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    continue-on-error: true
    timeout-minutes: 355

    strategy:
      fail-fast: false
      max-parallel: 8   # âœ… mniej kolejkowania / stabilniej (zmieÅ„ na 6-10 wg limitÃ³w)
      matrix:
        shard: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]  # âœ… 20 shardÃ³w

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Ensure zip/unzip
        run: |
          sudo apt-get update
          sudo apt-get install -y zip unzip

      - name: Unpack cache.zip (if exists)
        run: |
          mkdir -p data
          if [ -f data/cache.zip ]; then
            unzip -o data/cache.zip -d data
          else
            echo "No cache.zip found (first run)."
          fi

      # ===================== RUN =====================
      - name: Run watcher (sharded)
        continue-on-error: true
        env:
          PYTHONUNBUFFERED: "1"

          # ðŸ”¥ SHARDING
          SHARD_TOTAL: "20"
          SHARD_INDEX: "${{ matrix.shard }}"

          # soft stop BEFORE github hard kill
          RUN_DEADLINE_MIN: "345"

          # wydajnoÅ›Ä‡
          CONCURRENT_GMINY: "8"
          CONCURRENT_REQUESTS: "28"
          LIMIT_PER_HOST: "4"
          RATE_MIN_DELAY: "0.3"

          # âœ… mniej I/O
          CHECKPOINT_EVERY_SEC: "900"
        run: |
          python bip_watcher.py

      # ===================== SAFETY CACHE (NO HEREDOC) =====================
      - name: Ensure cache.json exists
        if: always()
        run: |
          mkdir -p data
          if [ ! -f data/cache.json ]; then
            echo "cache.json missing - creating placeholder"
            python -c "import json,os; os.makedirs('data',exist_ok=True); cache={'schema':10,'urls_seen':{},'content_seen':{},'gmina_seeds':{},'page_fprints':{},'gmina_frontiers':{},'gmina_retry':{}}; open('data/cache.json','w',encoding='utf-8').write(json.dumps(cache,ensure_ascii=False,indent=2))"
          fi

      - name: Pack shard cache
        if: always()
        run: |
          rm -f "data/cache_shard_${{ matrix.shard }}.zip"
          cd data
          zip -9 "cache_shard_${{ matrix.shard }}.zip" cache.json
          cd ..

      - name: Upload shard outputs (artifacts)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-output-shard-${{ matrix.shard }}
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache_shard_${{ matrix.shard }}.zip
            data/cache.json
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv

  merge:
    name: Merge caches + publish report
    runs-on: ubuntu-latest
    needs: [scan]
    if: always()
    timeout-minutes: 60

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Download shard artifacts
        if: always()
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge caches (best-effort)
        if: always()
        run: |
          python - <<'PY'
          import os, json, zipfile, sys

          def load_zip(zp):
              with zipfile.ZipFile(zp, "r") as z:
                  with z.open("cache.json") as f:
                      return json.load(f)

          def iso_max(a, b):
              return a if (a or "") >= (b or "") else b

          # znajdÅº wszystkie shard zipy
          zips = []
          for r, _, fs in os.walk("artifacts"):
              for f in fs:
                  if f.startswith("cache_shard_") and f.endswith(".zip"):
                      zips.append(os.path.join(r, f))
          zips.sort()

          if not zips:
              print("No shard caches found -> creating empty cache", file=sys.stderr)
              merged = {
                  "schema": 10,
                  "urls_seen": {},
                  "content_seen": {},
                  "gmina_seeds": {},
                  "page_fprints": {},
                  "gmina_frontiers": {},
                  "gmina_retry": {}
              }
          else:
              merged = None
              used = 0
              for zp in zips:
                  try:
                      c = load_zip(zp)
                  except Exception as e:
                      print(f"SKIP bad zip {zp}: {e}", file=sys.stderr)
                      continue

                  if merged is None:
                      merged = c
                      used = 1
                      continue

                  ua = merged.get("urls_seen", {}) or {}
                  ub = c.get("urls_seen", {}) or {}
                  for k, v in ub.items():
                      ua[k] = iso_max(ua.get(k, ""), v)
                  merged["urls_seen"] = ua

                  merged["content_seen"] = {**(merged.get("content_seen", {}) or {}), **(c.get("content_seen", {}) or {})}
                  merged["gmina_seeds"] = {**(merged.get("gmina_seeds", {}) or {}), **(c.get("gmina_seeds", {}) or {})}
                  merged["page_fprints"] = {**(merged.get("page_fprints", {}) or {}), **(c.get("page_fprints", {}) or {})}
                  merged["gmina_frontiers"] = {**(merged.get("gmina_frontiers", {}) or {}), **(c.get("gmina_frontiers", {}) or {})}
                  merged["gmina_retry"] = {**(merged.get("gmina_retry", {}) or {}), **(c.get("gmina_retry", {}) or {})}

                  used += 1

              if merged is None:
                  merged = {
                      "schema": 10,
                      "urls_seen": {},
                      "content_seen": {},
                      "gmina_seeds": {},
                      "page_fprints": {},
                      "gmina_frontiers": {},
                      "gmina_retry": {}
                  }

              print(f"Merged shards used={used}/{len(zips)}")

          os.makedirs("data", exist_ok=True)

          with open("data/cache.json", "w", encoding="utf-8") as f:
              json.dump(merged, f, ensure_ascii=False)

          with zipfile.ZipFile("data/cache.zip", "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
              z.write("data/cache.json", arcname="cache.json")

          print("OK: data/cache.json + data/cache.zip written")
          PY

      - name: Upload merged artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-merged-report
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache.zip
            data/cache.json

      - name: Commit merged cache back to repo
        if: always()
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git pull --rebase || true
          git add data/cache.zip data/cache.json || true
          git commit -m "Merge shard caches [skip ci]" || echo "Nothing to commit"
          git push || true
