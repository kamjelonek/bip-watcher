name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"
  workflow_dispatch: {}

concurrency:
  group: bip-watcher
  cancel-in-progress: false

permissions:
  contents: write
  issues: write

jobs:
  init_cache_store:
    name: Init cache-store branch (single)
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repo (main)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Create cache-store branch if missing
        shell: bash
        run: |
          set -e
          git fetch origin +refs/heads/cache-store:refs/remotes/origin/cache-store || true

          if git show-ref --verify --quiet refs/remotes/origin/cache-store; then
            echo "✅ cache-store already exists"
            exit 0
          fi

          echo "ℹ️ cache-store missing -> creating"
          git checkout --orphan cache-store

          # czyścimy working tree bez git rm (mniej kapryśne)
          rm -rf ./* ./.gitignore || true

          mkdir -p shards master
          echo "cache-store initialized" > README.md

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add README.md shards master
          git commit -m "Initialize cache-store [skip ci]"

          # push z retry na wypadek równoległego runu
          for i in 1 2 3 4 5; do
            git push origin cache-store && break
            echo "⚠️ Push retry #$i ..."
            sleep $((i*2))
          done

  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    continue-on-error: true
    timeout-minutes: 355
    needs: [init_cache_store]

    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        shard: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57]

    steps:
      - name: Checkout repo (main)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Checkout cache-store branch into ./cache-store
        uses: actions/checkout@v4
        with:
          ref: cache-store
          path: cache-store
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ===================== BUILD data/cache.json FROM cache-store =====================
      - name: Build cache.json from committed shards (and optional master zip parts)
        shell: bash
        run: |
          set -e
          mkdir -p data

          python - <<'PY'
          import json, glob, gzip, zipfile, shutil
          from pathlib import Path

          DATA_DIR = Path("data")
          STORE_DIR = Path("cache-store")
          SHARDS_DIR = STORE_DIR / "shards"
          MASTER_DIR = STORE_DIR / "master"

          REQUIRED = ["schema","urls_seen","content_seen","gmina_seeds","page_fprints","gmina_frontiers","gmina_retry","dead_urls"]

          def empty_cache():
              return {
                  "schema": 10,
                  "urls_seen": {},
                  "content_seen": {},
                  "gmina_seeds": {},
                  "page_fprints": {},
                  "gmina_frontiers": {},
                  "gmina_retry": {},
                  "dead_urls": {},
              }

          def ensure_keys(d):
              if not isinstance(d, dict):
                  d = {}
              for k in REQUIRED:
                  if k not in d:
                      d[k] = {} if k != "schema" else 10
              if not isinstance(d.get("schema"), int):
                  d["schema"] = 10
              return d

          merged = empty_cache()

          # 1) Optional: reconstruct master cache.zip from committed parts (fast bootstrap)
          parts = sorted(glob.glob(str(MASTER_DIR / "cache.zip.part-*")))
          if parts:
              zip_path = DATA_DIR / "cache.zip"
              with open(zip_path, "wb") as out:
                  for p in parts:
                      with open(p, "rb") as f:
                          shutil.copyfileobj(f, out)
              try:
                  with zipfile.ZipFile(zip_path, "r") as z:
                      z.extractall(DATA_DIR)
                  cj = DATA_DIR / "cache.json"
                  if cj.exists():
                      merged = ensure_keys(json.loads(cj.read_text(encoding="utf-8")))
                  print(f"✅ Bootstrapped from master zip parts: {len(parts)} parts")
              except Exception as e:
                  print("⚠️ Failed to use master zip parts:", e)

          # 2) Merge all committed shard gz files (source of truth)
          shard_files = sorted(glob.glob(str(SHARDS_DIR / "cache_shard_*.json.gz")))
          if not shard_files:
              print("ℹ️ No committed shards found yet -> starting fresh cache.json")
          else:
              for sf in shard_files:
                  try:
                      with gzip.open(sf, "rt", encoding="utf-8") as f:
                          data = ensure_keys(json.load(f))
                  except Exception as e:
                      print("⚠️ Failed reading shard:", sf, e)
                      continue

                  for k in merged:
                      if k == "schema":
                          continue
                      if k not in data:
                          continue
                      if isinstance(merged[k], dict) and isinstance(data[k], dict):
                          if k == "urls_seen":
                              # zachowaj nowszy timestamp
                              for h, ts in data[k].items():
                                  prev = merged[k].get(h, "")
                                  if (ts or "") >= (prev or ""):
                                      merged[k][h] = ts
                          elif k == "dead_urls":
                              for gkey, urls in data[k].items():
                                  merged[k].setdefault(gkey, [])
                                  seen = set(merged[k][gkey])
                                  for u in urls:
                                      if u not in seen:
                                          merged[k][gkey].append(u)
                                          seen.add(u)
                          else:
                              # last-write-wins (działa); semantyczny merge content_seen można dodać później
                              merged[k].update(data[k])
                      else:
                          merged[k] = data[k]

              print(f"✅ Merged committed shards: {len(shard_files)}")

          DATA_DIR.mkdir(parents=True, exist_ok=True)
          (DATA_DIR / "cache.json").write_text(json.dumps(merged, ensure_ascii=False, indent=2), encoding="utf-8")
          print("✅ data/cache.json ready")
          PY

      # ===================== RUN =====================
      - name: Run watcher (sharded)
        continue-on-error: true
        env:
          PYTHONUNBUFFERED: "1"
          SHARD_TOTAL: "58"
          SHARD_INDEX: "${{ matrix.shard }}"
          RUN_DEADLINE_MIN: "345"
          CONCURRENT_GMINY: "20"
          CONCURRENT_REQUESTS: "80"
          LIMIT_PER_HOST: "8"
          RATE_MIN_DELAY: "0.1"
          CHECKPOINT_EVERY_SEC: "900"
        run: |
          python bip_watcher.py

      # ===================== COMMIT THIS SHARD TO cache-store =====================
      - name: Compress shard and commit to cache-store
        if: always()
        shell: bash
        env:
          SHARD_INDEX: "${{ matrix.shard }}"
        run: |
          set -e
          ls -lah data || true

          if [ ! -f "data/cache_shard_${SHARD_INDEX}.json" ]; then
            echo "ℹ️ No shard file to commit: data/cache_shard_${SHARD_INDEX}.json"
            exit 0
          fi

          mkdir -p cache-store/shards

          python - <<'PY'
          import os, gzip, shutil
          shard = os.environ["SHARD_INDEX"]
          src = f"data/cache_shard_{shard}.json"
          dst = f"cache-store/shards/cache_shard_{shard}.json.gz"
          with open(src, "rb") as f_in, gzip.open(dst, "wb", compresslevel=9) as f_out:
              shutil.copyfileobj(f_in, f_out)
          print("✅ Written:", dst)
          PY

          cd cache-store
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add shards/cache_shard_${SHARD_INDEX}.json.gz
          git commit -m "Update shard ${SHARD_INDEX} [skip ci]" || echo "No changes to commit"

          for i in 1 2 3 4 5 6 7 8 9 10; do
            git pull --rebase && git push && break
            echo "⚠️ Push retry #$i ..."
            sleep $((i*2))
          done

      # ===================== OPTIONAL: keep per-run artifacts (debug/logs) =====================
      - name: Upload shard outputs (logs)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-shard-${{ matrix.shard }}
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv

  merge:
    name: Merge caches + publish master zip parts
    runs-on: ubuntu-latest
    needs: [scan]
    if: always()
    timeout-minutes: 60

    steps:
      - name: Checkout cache-store
        uses: actions/checkout@v4
        with:
          ref: cache-store
          path: cache-store
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Merge committed shards into master cache.json + cache.zip
        shell: bash
        run: |
          set -e
          mkdir -p data

          python - <<'PY'
          import json, glob, gzip, zipfile
          from pathlib import Path

          STORE = Path("cache-store")
          SHARDS = STORE / "shards"
          DATA = Path("data")

          merged = {
              "schema": 10,
              "urls_seen": {},
              "content_seen": {},
              "gmina_seeds": {},
              "page_fprints": {},
              "gmina_frontiers": {},
              "gmina_retry": {},
              "dead_urls": {}
          }

          shard_files = sorted(glob.glob(str(SHARDS / "cache_shard_*.json.gz")))
          if not shard_files:
              print("No committed shards found -> writing empty master")
          else:
              for sf in shard_files:
                  try:
                      with gzip.open(sf, "rt", encoding="utf-8") as f:
                          data = json.load(f)
                  except Exception as e:
                      print("Error reading", sf, e)
                      continue

                  for k in merged:
                      if k == "schema":
                          continue
                      if k not in data:
                          continue
                      if isinstance(merged[k], dict) and isinstance(data[k], dict):
                          if k == "urls_seen":
                              for h, ts in data[k].items():
                                  prev = merged[k].get(h, "")
                                  if (ts or "") >= (prev or ""):
                                      merged[k][h] = ts
                          elif k == "dead_urls":
                              for gkey, urls in data[k].items():
                                  merged[k].setdefault(gkey, [])
                                  seen = set(merged[k][gkey])
                                  for u in urls:
                                      if u not in seen:
                                          merged[k][gkey].append(u)
                                          seen.add(u)
                          else:
                              merged[k].update(data[k])
                      else:
                          merged[k] = data[k]

              print("Merged committed shards:", len(shard_files))

          DATA.mkdir(parents=True, exist_ok=True)
          (DATA / "cache.json").write_text(json.dumps(merged, ensure_ascii=False, indent=2), encoding="utf-8")

          with zipfile.ZipFile(DATA / "cache.zip", "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
              z.write(DATA / "cache.json", arcname="cache.json")

          print("OK: data/cache.json + data/cache.zip created")
          PY

      - name: Split cache.zip into <100MB parts and commit to cache-store
        if: always()
        shell: bash
        run: |
          set -e
          mkdir -p cache-store/master

          rm -f cache-store/master/cache.zip.part-* || true
          split -b 95m data/cache.zip cache-store/master/cache.zip.part-

          cd cache-store
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add master/cache.zip.part-*
          git commit -m "Update master cache zip parts [skip ci]" || echo "No changes to commit"

          for i in 1 2 3 4 5 6 7 8 9 10; do
            git pull --rebase && git push && break
            echo "⚠️ Push retry #$i ..."
            sleep $((i*2))
          done

      - name: Upload master cache artifact (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-master-cache
          retention-days: 30
          path: |
            data/cache.json
            data/cache.zip
