name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"
  workflow_dispatch: {}

concurrency:
  group: bip-watcher
  cancel-in-progress: false

permissions:
  contents: write
  issues: write

jobs:
  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    continue-on-error: true
    timeout-minutes: 355

    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        shard: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ===================== POBIERANIE GŁÓWNEGO CACHE Z ARTEFAKTÓW =====================
      - name: Download master cache (if exists)
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: bip-master-cache
          path: data

      - name: Unpack master cache if downloaded
        run: |
          if [ -f data/cache.zip ]; then
            unzip -o data/cache.zip -d data
            echo "✅ Master cache loaded"
          else
            echo "ℹ️ No master cache – starting fresh"
          fi

      # ===================== PRZYGOTOWANIE CACHE (scalanie starych shardów) =====================
      - name: Prepare cache (merge shards if any)
        run: |
          mkdir -p data
          # Sprawdź, czy istnieją pliki shardowe (np. z poprzednich nieudanych uruchomień)
          shard_files=$(ls data/cache_shard_*.json 2>/dev/null || true)
          if [ -n "$shard_files" ]; then
            echo "Znaleziono pliki shardowe – scalanie..."
            python - <<'PY'
          import os, json, glob

          def iso_max(a, b):
              return a if (a or "") >= (b or "") else b

          merged = {
              "schema": 10,
              "urls_seen": {},
              "content_seen": {},
              "gmina_seeds": {},
              "page_fprints": {},
              "gmina_frontiers": {},
              "gmina_retry": {},
              "dead_urls": {}
          }

          # Wczytaj istniejący cache.json jeśli istnieje
          if os.path.exists("data/cache.json"):
              with open("data/cache.json", "r") as f:
                  try:
                      merged = json.load(f)
                  except:
                      pass
              # Upewnij się, że wszystkie klucze są
              for k in list(merged.keys()):
                  if k not in merged:
                      merged[k] = {}

          # Znajdź wszystkie pliki shardowe
          shard_files = glob.glob("data/cache_shard_*.json")
          for f in shard_files:
              with open(f, "r") as fp:
                  data = json.load(fp)
              for k in merged:
                  if k == "schema": continue
                  if k not in data: continue
                  if isinstance(merged[k], dict) and isinstance(data[k], dict):
                      if k == "urls_seen":
                          for url, ts in data[k].items():
                              if url not in merged[k] or iso_max(merged[k].get(url, ""), ts) == ts:
                                  merged[k][url] = ts
                      elif k == "dead_urls":
                          for gkey, urls in data[k].items():
                              if gkey not in merged[k]:
                                  merged[k][gkey] = []
                              existing = set(merged[k][gkey])
                              for u in urls:
                                  if u not in existing:
                                      merged[k][gkey].append(u)
                      else:
                          merged[k].update(data[k])
                  else:
                      merged[k] = data[k]

          # Zapisz scalony cache.json
          with open("data/cache.json", "w") as f:
              json.dump(merged, f, indent=2)

          # Spakuj do cache.zip
          import zipfile
          with zipfile.ZipFile("data/cache.zip", "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
              z.write("data/cache.json", arcname="cache.json")

          # Usuń pliki shardowe
          for f in shard_files:
              os.remove(f)
          print("✅ Scalono i spakowano cache, usunięto pliki shardowe")
          PY
          else
            echo "Brak plików shardowych – używam istniejącego cache (jeśli jest)"
          fi

      # ===================== RUN =====================
      - name: Run watcher (sharded)
        continue-on-error: true
        env:
          PYTHONUNBUFFERED: "1"
          SHARD_TOTAL: "58"
          SHARD_INDEX: "${{ matrix.shard }}"
          RUN_DEADLINE_MIN: "345"
          CONCURRENT_GMINY: "20"
          CONCURRENT_REQUESTS: "80"
          LIMIT_PER_HOST: "8"
          RATE_MIN_DELAY: "0.1"
          CHECKPOINT_EVERY_SEC: "900"
        run: |
          python bip_watcher.py

      # ===================== WYSYŁKA BACKUPU SHARDÓW =====================
      - name: Upload shard outputs (logs and shard cache)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-shard-${{ matrix.shard }}
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache_shard_${{ matrix.shard }}.json
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv

  merge:
    name: Merge caches + publish report
    runs-on: ubuntu-latest
    needs: [scan]
    if: always()
    timeout-minutes: 60

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Download all shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge shard caches
        run: |
          python - <<'PY'
          import os, json, glob, zipfile, shutil

          def iso_max(a, b):
              return a if (a or "") >= (b or "") else b

          merged = {
              "schema": 10,
              "urls_seen": {},
              "content_seen": {},
              "gmina_seeds": {},
              "page_fprints": {},
              "gmina_frontiers": {},
              "gmina_retry": {},
              "dead_urls": {}
          }

          # Jeśli istnieje data/cache.json z poprzedniego merge (np. z repo), wczytaj go
          if os.path.exists("data/cache.json"):
              with open("data/cache.json", "r") as f:
                  try:
                      merged = json.load(f)
                  except:
                      pass
              for k in list(merged.keys()):
                  if k not in merged:
                      merged[k] = {}

          # Przeszukaj pobrane artefakty w poszukiwaniu plików cache_shard_*.json
          shard_files = []
          for root, dirs, files in os.walk("artifacts"):
              for f in files:
                  if f.startswith("cache_shard_") and f.endswith(".json"):
                      shard_files.append(os.path.join(root, f))

          if not shard_files:
              print("No shard cache files found")
          else:
              for f in shard_files:
                  try:
                      with open(f, "r") as fp:
                          data = json.load(fp)
                  except Exception as e:
                      print(f"Error reading {f}: {e}")
                      continue

                  for k in merged:
                      if k == "schema": continue
                      if k not in data: continue
                      if isinstance(merged[k], dict) and isinstance(data[k], dict):
                          if k == "urls_seen":
                              for url, ts in data[k].items():
                                  if url not in merged[k] or iso_max(merged[k].get(url, ""), ts) == ts:
                                      merged[k][url] = ts
                          elif k == "dead_urls":
                              for gkey, urls in data[k].items():
                                  if gkey not in merged[k]:
                                      merged[k][gkey] = []
                                  existing = set(merged[k][gkey])
                                  for u in urls:
                                      if u not in existing:
                                          merged[k][gkey].append(u)
                          else:
                              merged[k].update(data[k])
                      else:
                          merged[k] = data[k]

              print(f"Merged {len(shard_files)} shard files")

          # Zapisz scalony cache.json
          os.makedirs("data", exist_ok=True)
          with open("data/cache.json", "w") as f:
              json.dump(merged, f, indent=2)

          # Spakuj do cache.zip
          with zipfile.ZipFile("data/cache.zip", "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
              z.write("data/cache.json", arcname="cache.json")

          print("OK: data/cache.json and data/cache.zip updated")
          PY

      # ===================== WYSYŁKA GŁÓWNEGO CACHE JAKO ARTEFAKTU =====================
      - name: Upload master cache
        uses: actions/upload-artifact@v4
        with:
          name: bip-master-cache
          retention-days: 30
          path: |
            data/cache.json
            data/cache.zip

      - name: Upload merged report (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-merged-report
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/summary_report.txt
            data/log.csv
