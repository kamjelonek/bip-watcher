name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"
  workflow_dispatch: {}

concurrency:
  group: bip-watcher
  cancel-in-progress: false

permissions:
  contents: write
  issues: write

jobs:
  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    continue-on-error: true
    timeout-minutes: 355

    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        shard: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Ensure zip/unzip (optional)
        run: |
          sudo apt-get update
          sudo apt-get install -y zip unzip

      # ===================== RUN =====================
      - name: Run watcher (sharded)
        continue-on-error: true
        env:
          PYTHONUNBUFFERED: "1"
          SHARD_TOTAL: "58"
          SHARD_INDEX: "${{ matrix.shard }}"
          RUN_DEADLINE_MIN: "345"
          CONCURRENT_GMINY: "8"
          CONCURRENT_REQUESTS: "28"
          LIMIT_PER_HOST: "4"
          RATE_MIN_DELAY: "0.3"
          CHECKPOINT_EVERY_SEC: "900"
        run: |
          python bip_watcher.py

      # ===================== BACKUP ARTIFACTS (optional) =====================
      - name: Ensure cache.json exists (fallback)
        if: always()
        run: |
          mkdir -p data
          if [ ! -f data/cache.json ]; then
            echo "cache.json missing - creating placeholder"
            python -c "import json,os; os.makedirs('data',exist_ok=True); cache={'schema':10,'urls_seen':{},'content_seen':{},'gmina_seeds':{},'page_fprints':{},'gmina_frontiers':{},'gmina_retry':{}}; open('data/cache.json','w',encoding='utf-8').write(json.dumps(cache,ensure_ascii=False,indent=2))"
          fi

      - name: Pack shard cache (backup)
        if: always()
        run: |
          rm -f "data/cache_shard_${{ matrix.shard }}.zip"
          cd data
          zip -9 "cache_shard_${{ matrix.shard }}.zip" cache.json
          cd ..

      - name: Upload shard outputs (artifacts)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-output-shard-${{ matrix.shard }}
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache_shard_${{ matrix.shard }}.zip
            data/cache.json
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv

  merge:
    name: Merge caches + publish report
    runs-on: ubuntu-latest
    needs: [scan]
    if: always()
    timeout-minutes: 60

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Merge shard caches from repository
        run: |
          python - <<'PY'
          import os, json, glob, sys

          def iso_max(a, b):
              return a if (a or "") >= (b or "") else b

          merged = {
              "schema": 10,
              "urls_seen": {},
              "content_seen": {},
              "gmina_seeds": {},
              "page_fprints": {},
              "gmina_frontiers": {},
              "gmina_retry": {}
          }

          # wczytaj istniejący cache.json (jeśli istnieje)
          if os.path.exists("data/cache.json"):
              with open("data/cache.json", "r") as f:
                  try:
                      merged = json.load(f)
                  except:
                      pass

          shard_files = glob.glob("data/cache_shard_*.json")
          if not shard_files:
              print("No shard cache files found", file=sys.stderr)
          else:
              for f in shard_files:
                  try:
                      with open(f, "r") as fp:
                          data = json.load(fp)
                  except Exception as e:
                      print(f"Error reading {f}: {e}", file=sys.stderr)
                      continue

                  # scalanie pól
                  for k in merged:
                      if k == "schema":
                          continue
                      if k not in data:
                          continue
                      if isinstance(merged[k], dict) and isinstance(data[k], dict):
                          if k == "urls_seen":
                              for url, ts in data[k].items():
                                  if url not in merged[k] or iso_max(merged[k].get(url, ""), ts) == ts:
                                      merged[k][url] = ts
                          else:
                              merged[k].update(data[k])
                      else:
                          merged[k] = data[k]

              print(f"Merged {len(shard_files)} shard files")

          # zapisz do cache.json
          os.makedirs("data", exist_ok=True)
          with open("data/cache.json", "w") as f:
              json.dump(merged, f, indent=2)

          # opcjonalnie usuń pliki shardowe
          for f in shard_files:
              os.remove(f)

          # utwórz cache.zip (opcjonalnie)
          import zipfile
          with zipfile.ZipFile("data/cache.zip", "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
              z.write("data/cache.json", arcname="cache.json")

          print("OK: data/cache.json and data/cache.zip updated")
          PY

      - name: Commit merged cache back to repo
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git pull --rebase || true
          git add data/cache.json data/cache.zip
          # Usuń pliki shardowe z repo (jeśli istnieją)
          git rm --ignore-unmatch data/cache_shard_*.json
          git commit -m "Merge shard caches [skip ci]" || echo "Nothing to commit"
          git push || true

      - name: Upload merged artifact (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-merged-report
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache.json
            data/cache.zip
