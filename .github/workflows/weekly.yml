name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"
  workflow_dispatch: {}

concurrency:
  group: bip-watcher
  cancel-in-progress: false

permissions:
  contents: write
  issues: write

jobs:
  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    timeout-minutes: 355

    strategy:
      fail-fast: false
      matrix:
        shard: [0,1,2,3,4,5,6,7,8,9,10,11]   # ðŸ”¥ 12 shardÃ³w

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Ensure zip/unzip
        run: |
          sudo apt-get update
          sudo apt-get install -y zip unzip

      - name: Unpack cache.zip (if exists)
        run: |
          mkdir -p data
          if [ -f data/cache.zip ]; then
            unzip -o data/cache.zip -d data
          else
            echo "No cache.zip found (first run)."
          fi

      # ===================== RUN =====================
      - name: Run watcher (sharded)
        continue-on-error: true
        env:
          PYTHONUNBUFFERED: "1"

          # ðŸ”¥ SHARDING
          SHARD_TOTAL: "12"
          SHARD_INDEX: "${{ matrix.shard }}"

          # soft stop BEFORE github hard kill
          RUN_DEADLINE_MIN: "345"

          # Twoje szybkie wartoÅ›ci
          CONCURRENT_GMINY: "8"
          CONCURRENT_REQUESTS: "28"
          LIMIT_PER_HOST: "4"
          RATE_MIN_DELAY: "0.3"

          CHECKPOINT_EVERY_SEC: "60"
        run: |
          python bip_watcher.py

      # ===================== SAFETY CACHE =====================
      - name: Ensure cache.json exists
        if: always()
        run: |
          mkdir -p data
          if [ ! -f data/cache.json ]; then
            echo "cache.json missing - creating placeholder"
            python -c "import json,os; os.makedirs('data',exist_ok=True); cache={'schema':10,'urls_seen':{},'content_seen':{},'gmina_seeds':{},'page_fprints':{},'gmina_frontiers':{},'gmina_retry':{}}; open('data/cache.json','w',encoding='utf-8').write(json.dumps(cache,ensure_ascii=False,indent=2))"
          fi

      - name: Pack shard cache
        if: always()
        run: |
          rm -f "data/cache_shard_${{ matrix.shard }}.zip"
          cd data
          zip -9 "cache_shard_${{ matrix.shard }}.zip" cache.json
          cd ..

      - name: Upload shard outputs (artifacts)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-output-shard-${{ matrix.shard }}
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache_shard_${{ matrix.shard }}.zip
            data/cache.json
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv

  # ===================== MERGE =====================
  merge:
    name: Merge caches + publish report
    runs-on: ubuntu-latest
    needs: [scan]
    if: always()
    timeout-minutes: 60

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Download shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge caches
        run: |
          python - <<'PY'
          import os, json, zipfile

          def load(zp):
              with zipfile.ZipFile(zp,"r") as z:
                  with z.open("cache.json") as f:
                      return json.load(f)

          def iso_max(a,b):
              return a if (a or "") >= (b or "") else b

          zips=[]
          for r,_,fs in os.walk("artifacts"):
              for f in fs:
                  if f.startswith("cache_shard_") and f.endswith(".zip"):
                      zips.append(os.path.join(r,f))
          zips.sort()

          if not zips:
              raise SystemExit("No shard caches found")

          merged=load(zips[0])

          for zp in zips[1:]:
              c=load(zp)

              ua=merged.get("urls_seen",{})
              ub=c.get("urls_seen",{})
              for k,v in ub.items():
                  ua[k]=iso_max(ua.get(k,""),v)
              merged["urls_seen"]=ua

              merged["content_seen"]={**merged.get("content_seen",{}), **c.get("content_seen",{})}
              merged["gmina_seeds"]={**merged.get("gmina_seeds",{}), **c.get("gmina_seeds",{})}
              merged["page_fprints"]={**merged.get("page_fprints",{}), **c.get("page_fprints",{})}
              merged["gmina_frontiers"]={**merged.get("gmina_frontiers",{}), **c.get("gmina_frontiers",{})}
              merged["gmina_retry"]={**merged.get("gmina_retry",{}), **c.get("gmina_retry",{})}

          os.makedirs("data",exist_ok=True)
          with open("data/cache.json","w",encoding="utf-8") as f:
              json.dump(merged,f,ensure_ascii=False)

          with zipfile.ZipFile("data/cache.zip","w",compression=zipfile.ZIP_DEFLATED,compresslevel=9) as z:
              z.write("data/cache.json",arcname="cache.json")

          print("Merged",len(zips),"shards")
          PY

      - name: Upload merged artifact
        uses: actions/upload-artifact@v4
        with:
          name: bip-merged-report
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache.zip
            data/cache.json

      - name: Commit merged cache back to repo
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git pull --rebase || true
          git add data/cache.zip data/cache.json || true
          git commit -m "Merge shard caches [skip ci]" || echo "Nothing to commit"
          git push || true
