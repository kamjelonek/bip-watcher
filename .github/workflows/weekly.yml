name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"   # poniedziałek 05:00 UTC
  workflow_dispatch: {}

concurrency:
  group: bip-watcher
  cancel-in-progress: false

permissions:
  contents: write
  issues: write

jobs:
  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    timeout-minutes: 355

    strategy:
      fail-fast: false
      matrix:
        shard: [0, 1, 2, 3, 4, 5, 6, 7]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Ensure zip/unzip
        run: |
          sudo apt-get update
          sudo apt-get install -y zip unzip

      - name: Unpack cache.zip (if exists)
        run: |
          mkdir -p data
          if [ -f data/cache.zip ]; then
            unzip -o data/cache.zip -d data
          else
            echo "No cache.zip found (first run)."
          fi

      - name: Run watcher (sharded)
        continue-on-error: true
        env:
          PYTHONUNBUFFERED: "1"

          # --- SHARDING ---
          SHARD_TOTAL: "8"
          SHARD_INDEX: "${{ matrix.shard }}"

          # --- RUN TIME BUDGET (miękki deadline w kodzie) ---
          RUN_DEADLINE_MIN: "345"

          # --- THROTTLING PER JOB (ważne przy 8 jobach naraz) ---
          CONCURRENT_GMINY: "4"
          CONCURRENT_REQUESTS: "16"
          LIMIT_PER_HOST: "2"
          RATE_MIN_DELAY: "0.8"

          # checkpoint częściej, żeby nie tracić postępu przy timeoutach
          CHECKPOINT_EVERY_SEC: "30"
        run: |
          python bip_watcher.py

      - name: Ensure cache.json exists
        if: always()
        run: |
          mkdir -p data
          if [ ! -f data/cache.json ]; then
            echo "cache.json missing - creating minimal placeholder to allow merge."
            python - << 'PY'
            import json, os
            os.makedirs("data", exist_ok=True)
            cache = {
              "schema": 10,
              "urls_seen": {},
              "content_seen": {},
              "gmina_seeds": {},
              "page_fprints": {},
              "gmina_frontiers": {},
              "gmina_retry": {},
            }
            with open("data/cache.json", "w", encoding="utf-8") as f:
              json.dump(cache, f, ensure_ascii=False, indent=2)
            PY
          fi

      - name: Pack shard cache
        if: always()
        run: |
          rm -f "data/cache_shard_${{ matrix.shard }}.zip"
          cd data
          zip -9 "cache_shard_${{ matrix.shard }}.zip" cache.json
          cd ..

      - name: Shard summary to Step Summary
        if: always()
        run: |
          echo "## Shard ${{ matrix.shard }}" >> $GITHUB_STEP_SUMMARY
          if [ -f data/summary_report.txt ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -n 80 data/summary_report.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "- No summary_report.txt" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          for f in data/diag_gminy.csv data/diag_errors.csv data/log.csv; do
            if [ -f "$f" ]; then
              echo "- ✅ $(basename "$f") exists ($(wc -l < "$f") lines)" >> $GITHUB_STEP_SUMMARY
            else
              echo "- ❌ $(basename "$f") missing" >> $GITHUB_STEP_SUMMARY
            fi
          done

      - name: Upload shard outputs (artifacts)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-output-shard-${{ matrix.shard }}
          if-no-files-found: warn
          retention-days: 30
          path: |
            data/cache_shard_${{ matrix.shard }}.zip
            data/cache.json
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv

  merge:
    name: Merge caches + publish report
    runs-on: ubuntu-latest
    needs: [scan]
    if: always()
    timeout-minutes: 60

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Download all shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge cache.json from shards into main data/cache.zip
        run: |
          python - << 'PY'
          import os, json, zipfile

          def load_cache_from_zip(zpath):
            with zipfile.ZipFile(zpath, "r") as z:
              with z.open("cache.json") as f:
                return json.load(f)

          def iso_max(a, b):
            if not a: return b
            if not b: return a
            return a if a >= b else b

          def merge_content_seen(a, b):
            out = dict(a or {})
            for k, vb in (b or {}).items():
              va = out.get(k)
              if not isinstance(vb, dict):
                continue
              if not isinstance(va, dict):
                out[k] = vb
                continue

              outv = dict(va)
              outv["found_at"] = iso_max(va.get("found_at",""), vb.get("found_at",""))
              outv["last_checked"] = iso_max(va.get("last_checked",""), vb.get("last_checked",""))

              if (vb.get("last_checked","") or "") >= (va.get("last_checked","") or ""):
                for fld in ("url","title","gmina","etag","last_modified","att_sig"):
                  if vb.get(fld):
                    outv[fld] = vb.get(fld)

              kwa = set(va.get("keywords") or [])
              kwb = set(vb.get("keywords") or [])
              outv["keywords"] = sorted(kwa | kwb)

              pri = {"ZMIANA": 4, "NOWE": 3, "HIT": 2, "NO_MATCH": 1, "ALIAS": 0}
              sa = va.get("status") or ""
              sb = vb.get("status") or ""
              outv["status"] = sa if pri.get(sa, 0) >= pri.get(sb, 0) else sb

              out[k] = outv
            return out

          def merge_dict_latest_ts(a, b, ts_field="ts"):
            out = dict(a or {})
            for k, vb in (b or {}).items():
              va = out.get(k)
              if not isinstance(vb, dict):
                continue
              if not isinstance(va, dict):
                out[k] = vb
                continue
              tsa = va.get(ts_field, "") or ""
              tsb = vb.get(ts_field, "") or ""
              out[k] = va if tsa >= tsb else vb
            return out

          shard_zips = []
          for root, _, files in os.walk("artifacts"):
            for fn in files:
              if fn.startswith("cache_shard_") and fn.endswith(".zip"):
                shard_zips.append(os.path.join(root, fn))
          shard_zips.sort()

          if not shard_zips:
            raise SystemExit("No shard cache zips found in artifacts/")

          caches = [load_cache_from_zip(p) for p in shard_zips]
          merged = caches[0]

          for c in caches[1:]:
            merged["schema"] = max(int(merged.get("schema", 0) or 0), int(c.get("schema", 0) or 0))

            ua = merged.get("urls_seen") or {}
            ub = c.get("urls_seen") or {}
            out_urls = dict(ua)
            for k, ts in ub.items():
              out_urls[k] = iso_max(out_urls.get(k, ""), ts)
            merged["urls_seen"] = out_urls

            merged["content_seen"] = merge_content_seen(merged.get("content_seen") or {}, c.get("content_seen") or {})

            merged["gmina_seeds"] = merge_dict_latest_ts(merged.get("gmina_seeds") or {}, c.get("gmina_seeds") or {}, ts_field="ts")
            merged["page_fprints"] = merge_dict_latest_ts(merged.get("page_fprints") or {}, c.get("page_fprints") or {}, ts_field="ts")

            ga = merged.get("gmina_frontiers") or {}
            gb = c.get("gmina_frontiers") or {}
            out_gf = dict(ga)
            for k, vb in gb.items():
              va = out_gf.get(k)
              if not isinstance(vb, list):
                continue
              if not isinstance(va, list) or len(vb) > len(va):
                out_gf[k] = vb
            merged["gmina_frontiers"] = out_gf

            ra = merged.get("gmina_retry") or {}
            rb = c.get("gmina_retry") or {}
            out_gr = dict(ra)
            for k, vb in rb.items():
              va = out_gr.get(k)
              if not isinstance(vb, list):
                continue
              if not isinstance(va, list):
                out_gr[k] = vb
              else:
                s = set(va)
                for u in vb:
                  if u not in s:
                    va.append(u)
                    s.add(u)
                  if len(va) >= 60000:
                    break
                out_gr[k] = va
            merged["gmina_retry"] = out_gr

          os.makedirs("data", exist_ok=True)
          with open("data/cache.json", "w", encoding="utf-8") as f:
            json.dump(merged, f, ensure_ascii=False, indent=2)

          zpath = "data/cache.zip"
          if os.path.exists(zpath):
            os.remove(zpath)
          with zipfile.ZipFile(zpath, "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
            z.write("data/cache.json", arcname="cache.json")

          print(f"Merged {len(shard_zips)} shard caches -> {zpath}")
          PY

      - name: Merge CSVs (diag/log) + summary into data/merged_*
        if: always()
        run: |
          python - << 'PY'
          import os, glob, csv, shutil

          os.makedirs("data", exist_ok=True)

          def merge_csv(pattern, out_path):
            files = sorted(glob.glob(pattern, recursive=True))
            if not files:
              return 0, []
            header = None
            rows = 0
            used = []
            with open(out_path, "w", encoding="utf-8", newline="") as fout:
              w = None
              for fp in files:
                try:
                  with open(fp, "r", encoding="utf-8", newline="") as fin:
                    r = csv.reader(fin)
                    h = next(r, None)
                    if not h:
                      continue
                    if header is None:
                      header = h
                      w = csv.writer(fout)
                      w.writerow(header)
                    # jeśli header inny, próbujemy dalej, ale pomijamy
                    if h != header:
                      continue
                    for row in r:
                      w.writerow(row)
                      rows += 1
                    used.append(fp)
                except Exception:
                  continue
            return rows, used

          # diag_gminy.csv, diag_errors.csv, log.csv
          rows1, used1 = merge_csv("artifacts/**/diag_gminy.csv", "data/merged_diag_gminy.csv")
          rows2, used2 = merge_csv("artifacts/**/diag_errors.csv", "data/merged_diag_errors.csv")
          rows3, used3 = merge_csv("artifacts/**/log.csv", "data/merged_log.csv")

          # summary_report.txt: zbieramy w jeden plik (doklejamy shardami)
          sum_files = sorted(glob.glob("artifacts/**/summary_report.txt", recursive=True))
          merged_sum = "data/merged_summary_report.txt"
          if sum_files:
            with open(merged_sum, "w", encoding="utf-8") as out:
              for fp in sum_files:
                out.write("\n" + ("="*80) + "\n")
                out.write(f"FILE: {fp}\n")
                out.write(("="*80) + "\n")
                try:
                  with open(fp, "r", encoding="utf-8", errors="ignore") as fin:
                    out.write(fin.read())
                except Exception as e:
                  out.write(f"[read error] {e}\n")

          print("merged_diag_gminy_rows:", rows1)
          print("merged_diag_errors_rows:", rows2)
          print("merged_log_rows:", rows3)
          print("merged_summary_files:", len(sum_files))
          PY

      - name: Merge summary to Step Summary
        if: always()
        run: |
          echo "## Merge results" >> $GITHUB_STEP_SUMMARY
          for f in data/cache.zip data/cache.json data/merged_diag_gminy.csv data/merged_diag_errors.csv data/merged_log.csv data/merged_summary_report.txt; do
            if [ -f "$f" ]; then
              echo "- ✅ $(basename "$f") exists ($(wc -l < "$f" 2>/dev/null || echo "?") lines)" >> $GITHUB_STEP_SUMMARY
            else
              echo "- ❌ $(basename "$f") missing" >> $GITHUB_STEP_SUMMARY
            fi
          done

      - name: Upload merged report artifact (one-click download)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-merged-report
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache.zip
            data/cache.json
            data/merged_diag_gminy.csv
            data/merged_diag_errors.csv
            data/merged_log.csv
            data/merged_summary_report.txt

      - name: Commit merged cache.zip back to repo
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git pull --rebase || true

          git add data/cache.zip data/cache.json || true
          git status --porcelain

          git commit -m "Merge shard caches [skip ci]" || echo "Nothing to commit"
          git push || true
