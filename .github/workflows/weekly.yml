name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"
  workflow_dispatch: {}

concurrency:
  group: bip-watcher
  cancel-in-progress: false

permissions:
  contents: write
  issues: write

jobs:
  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    continue-on-error: true
    timeout-minutes: 355

    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        shard: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57]

    steps:
      - name: Checkout repo (main)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Ensure cache-store branch exists (create if missing)
        shell: bash
        run: |
          set -e
          git fetch origin +refs/heads/cache-store:refs/remotes/origin/cache-store || true
          if git show-ref --verify --quiet refs/remotes/origin/cache-store; then
            echo "✅ cache-store branch exists"
          else
            echo "ℹ️ cache-store branch missing -> creating empty branch"
            git checkout --orphan cache-store
            git rm -rf . || true
            mkdir -p shards master
            echo "cache-store initialized" > README.md
            git add README.md shards master
            git -c user.name="github-actions[bot]" -c user.email="github-actions[bot]@users.noreply.github.com" commit -m "Initialize cache-store"
            git push origin cache-store
            git checkout -
          fi

      - name: Checkout cache-store branch into ./cache-store
        uses: actions/checkout@v4
        with:
          ref: cache-store
          path: cache-store
          fetch-depth: 1

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ===================== BUILD data/cache.json FROM cache-store =====================
      - name: Build cache.json from committed shards (and optional master zip parts)
        shell: bash
        run: |
          set -e
          mkdir -p data

          python - <<'PY'
          import os, json, glob, gzip, zipfile, shutil
          from pathlib import Path

          DATA_DIR = Path("data")
          STORE_DIR = Path("cache-store")
          SHARDS_DIR = STORE_DIR / "shards"
          MASTER_DIR = STORE_DIR / "master"

          REQUIRED = ["schema","urls_seen","content_seen","gmina_seeds","page_fprints","gmina_frontiers","gmina_retry","dead_urls"]

          def empty_cache():
              return {
                  "schema": 10,
                  "urls_seen": {},
                  "content_seen": {},
                  "gmina_seeds": {},
                  "page_fprints": {},
                  "gmina_frontiers": {},
                  "gmina_retry": {},
                  "dead_urls": {},
              }

          def ensure_keys(d):
              for k in REQUIRED:
                  if k not in d:
                      d[k] = {} if k != "schema" else 10
              if not isinstance(d.get("schema"), int):
                  d["schema"] = 10
              return d

          def iso_max(a, b):
              return a if (a or "") >= (b or "") else b

          merged = empty_cache()

          # 1) Optional: reconstruct master cache.zip from committed parts (faster bootstrap)
          parts = sorted(glob.glob(str(MASTER_DIR / "cache.zip.part-*")))
          if parts:
              zip_path = DATA_DIR / "cache.zip"
              with open(zip_path, "wb") as out:
                  for p in parts:
                      with open(p, "rb") as f:
                          shutil.copyfileobj(f, out)
              try:
                  with zipfile.ZipFile(zip_path, "r") as z:
                      z.extractall(DATA_DIR)
                  cj = DATA_DIR / "cache.json"
                  if cj.exists():
                      merged = json.loads(cj.read_text(encoding="utf-8"))
                      merged = ensure_keys(merged)
                  print(f"✅ Bootstrapped from master zip parts: {len(parts)} parts")
              except Exception as e:
                  print("⚠️ Failed to use master zip parts:", e)

          # 2) Merge all committed shard gz files
          shard_files = sorted(glob.glob(str(SHARDS_DIR / "cache_shard_*.json.gz")))
          if not shard_files:
              print("ℹ️ No committed shards found yet (first run) -> starting fresh cache.json")
          else:
              for sf in shard_files:
                  try:
                      with gzip.open(sf, "rt", encoding="utf-8") as f:
                          data = json.load(f)
                  except Exception as e:
                      print("⚠️ Failed reading shard:", sf, e)
                      continue

                  data = ensure_keys(data)

                  for k in merged:
                      if k == "schema":
                          continue
                      if k not in data:
                          continue
                      if isinstance(merged[k], dict) and isinstance(data[k], dict):
                          if k == "urls_seen":
                              for h, ts in data[k].items():
                                  prev = merged[k].get(h, "")
                                  if (ts or "") >= (prev or ""):
                                      merged[k][h] = ts
                          elif k == "dead_urls":
                              for gkey, urls in data[k].items():
                                  merged[k].setdefault(gkey, [])
                                  seen = set(merged[k][gkey])
                                  for u in urls:
                                      if u not in seen:
                                          merged[k][gkey].append(u)
                                          seen.add(u)
                          else:
                              # NOTE: last-write-wins; działa, ale docelowo warto zrobić semantyczny merge content_seen
                              merged[k].update(data[k])
                      else:
                          merged[k] = data[k]

              print(f"✅ Merged committed shards: {len(shard_files)}")

          # Save data/cache.json (this is what bip_watcher.py reads)
          DATA_DIR.mkdir(parents=True, exist_ok=True)
          (DATA_DIR / "cache.json").write_text(json.dumps(merged, ensure_ascii=False, indent=2), encoding="utf-8")
          print("✅ data/cache.json ready:", (DATA_DIR / "cache.json"))
          PY

      # ===================== RUN =====================
      - name: Run watcher (sharded)
        continue-on-error: true
        env:
          PYTHONUNBUFFERED: "1"
          SHARD_TOTAL: "58"
          SHARD_INDEX: "${{ matrix.shard }}"
          RUN_DEADLINE_MIN: "345"
          CONCURRENT_GMINY: "20"
          CONCURRENT_REQUESTS: "80"
          LIMIT_PER_HOST: "8"
          RATE_MIN_DELAY: "0.1"
          CHECKPOINT_EVERY_SEC: "900"
        run: |
          python bip_watcher.py

      # ===================== COMMIT THIS SHARD TO cache-store =====================
      - name: Compress shard and commit to cache-store
        if: always()
        shell: bash
        env:
          SHARD_INDEX: "${{ matrix.shard }}"
        run: |
          set -e
          ls -lah data || true

          # shard json must exist; if missing (run failed early), skip commit
          if [ ! -f "data/cache_shard_${SHARD_INDEX}.json" ]; then
            echo "ℹ️ No shard file to commit: data/cache_shard_${SHARD_INDEX}.json"
            exit 0
          fi

          mkdir -p cache-store/shards

          python - <<'PY'
          import os, gzip, shutil
          shard = os.environ["SHARD_INDEX"]
          src = f"data/cache_shard_{shard}.json"
          dst = f"cache-store/shards/cache_shard_{shard}.json.gz"
          with open(src, "rb") as f_in, gzip.open(dst, "wb", compresslevel=9) as f_out:
              shutil.copyfileobj(f_in, f_out)
          print("✅ Written:", dst)
          PY

          cd cache-store
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add shards/cache_shard_${SHARD_INDEX}.json.gz
          git commit -m "Update shard ${SHARD_INDEX} [skip ci]" || echo "No changes to commit"

          # push with retry (handles parallel writers)
          for i in 1 2 3 4 5 6 7 8 9 10; do
            git pull --rebase && git push && break
            echo "⚠️ Push retry #$i ..."
            sleep $((i*2))
          done

      # ===================== OPTIONAL: keep per-run artifacts (debug/logs) =====================
      - name: Upload shard outputs (logs)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-shard-${{ matrix.shard }}
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv

  merge:
    name: Merge caches + publish master zip parts
    runs-on: ubuntu-latest
    needs: [scan]
    if: always()
    timeout-minutes: 60

    steps:
      - name: Checkout repo (main)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Checkout cache-store
        uses: actions/checkout@v4
        with:
          ref: cache-store
          path: cache-store
          fetch-depth: 1

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Merge committed shards into master cache.json + cache.zip
        shell: bash
        run: |
          set -e
          mkdir -p data

          python - <<'PY'
          import json, glob, gzip, zipfile
          from pathlib import Path

          STORE = Path("cache-store")
          SHARDS = STORE / "shards"
          DATA = Path("data")

          merged = {
              "schema": 10,
              "urls_seen": {},
              "content_seen": {},
              "gmina_seeds": {},
              "page_fprints": {},
              "gmina_frontiers": {},
              "gmina_retry": {},
              "dead_urls": {}
          }

          def iso_max(a, b):
              return a if (a or "") >= (b or "") else b

          shard_files = sorted(glob.glob(str(SHARDS / "cache_shard_*.json.gz")))
          if not shard_files:
              print("No committed shards found -> writing empty master")
          else:
              for sf in shard_files:
                  try:
                      with gzip.open(sf, "rt", encoding="utf-8") as f:
                          data = json.load(f)
                  except Exception as e:
                      print("Error reading", sf, e)
                      continue

                  for k in merged:
                      if k == "schema":
                          continue
                      if k not in data:
                          continue
                      if isinstance(merged[k], dict) and isinstance(data[k], dict):
                          if k == "urls_seen":
                              for h, ts in data[k].items():
                                  prev = merged[k].get(h, "")
                                  if (ts or "") >= (prev or ""):
                                      merged[k][h] = ts
                          elif k == "dead_urls":
                              for gkey, urls in data[k].items():
                                  merged[k].setdefault(gkey, [])
                                  seen = set(merged[k][gkey])
                                  for u in urls:
                                      if u not in seen:
                                          merged[k][gkey].append(u)
                                          seen.add(u)
                          else:
                              merged[k].update(data[k])
                      else:
                          merged[k] = data[k]

              print("Merged committed shards:", len(shard_files))

          DATA.mkdir(parents=True, exist_ok=True)
          (DATA / "cache.json").write_text(json.dumps(merged, ensure_ascii=False, indent=2), encoding="utf-8")

          with zipfile.ZipFile(DATA / "cache.zip", "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
              z.write(DATA / "cache.json", arcname="cache.json")

          print("OK: data/cache.json + data/cache.zip created")
          PY

      - name: Split cache.zip into <100MB parts and commit to cache-store
        if: always()
        shell: bash
        run: |
          set -e
          mkdir -p cache-store/master

          # remove old parts
          rm -f cache-store/master/cache.zip.part-* || true

          # split into 95MB chunks (safe under 100MB)
          split -b 95m data/cache.zip cache-store/master/cache.zip.part-

          cd cache-store
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add master/cache.zip.part-*
          git commit -m "Update master cache zip parts [skip ci]" || echo "No changes to commit"
          git pull --rebase || true
          git push || true

      # Optional: upload master cache as artifact for convenience (per-run)
      - name: Upload master cache artifact (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-master-cache
          retention-days: 30
          path: |
            data/cache.json
            data/cache.zip
