name: BIP Watcher Weekly

on:
  schedule:
    - cron: "0 5 * * 1"
  workflow_dispatch: {}

concurrency:
  group: bip-watcher
  cancel-in-progress: false

permissions:
  contents: write
  issues: write

jobs:
  scan:
    name: Scan shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    continue-on-error: true
    timeout-minutes: 355

    strategy:
      fail-fast: false
      max-parallel: 8
      matrix:
        shard: [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57]

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ===================== ROZPAKOWANIE GŁÓWNEGO CACHE =====================
      - name: Unpack cache.zip if exists
        run: |
          mkdir -p data
          if [ -f data/cache.zip ]; then
            unzip -o data/cache.zip -d data
            echo "✅ cache.zip unpacked to data/cache.json"
          else
            echo "ℹ️ No cache.zip found – starting fresh"
          fi

      # ===================== RUN =====================
      - name: Run watcher (sharded)
        continue-on-error: true
        env:
          PYTHONUNBUFFERED: "1"
          SHARD_TOTAL: "58"
          SHARD_INDEX: "${{ matrix.shard }}"
          RUN_DEADLINE_MIN: "345"
          CONCURRENT_GMINY: "20"
          CONCURRENT_REQUESTS: "80"
          LIMIT_PER_HOST: "8"
          RATE_MIN_DELAY: "0.1"
          CHECKPOINT_EVERY_SEC: "900"
        run: |
          python bip_watcher.py

      # ===================== WYSYŁKA ARTEFAKTÓW (LOGÓW I PLIKU SHARDOWEGO) =====================
      - name: Upload shard outputs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-output-shard-${{ matrix.shard }}
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache_shard_${{ matrix.shard }}.json
            data/summary_report.txt
            data/log.csv
            data/diag_gminy.csv
            data/diag_errors.csv

  merge:
    name: Merge caches + publish report
    runs-on: ubuntu-latest
    needs: [scan]
    if: always()
    timeout-minutes: 60

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Download all shard artifacts
        if: always()
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Merge shard caches
        run: |
          python - <<'PY'
          import os, json, glob, zipfile, shutil

          def iso_max(a, b):
              return a if (a or "") >= (b or "") else b

          merged = {
              "schema": 10,
              "urls_seen": {},
              "content_seen": {},
              "gmina_seeds": {},
              "page_fprints": {},
              "gmina_frontiers": {},
              "gmina_retry": {},
              "dead_urls": {}
          }

          # Wczytaj istniejący cache.json (jeśli istnieje)
          if os.path.exists("data/cache.json"):
              with open("data/cache.json", "r") as f:
                  try:
                      merged = json.load(f)
                  except:
                      pass
              # Upewnij się, że wszystkie klucze istnieją
              for k in ["urls_seen", "content_seen", "gmina_seeds", "page_fprints", "gmina_frontiers", "gmina_retry", "dead_urls"]:
                  if k not in merged:
                      merged[k] = {}

          # Przeszukaj pobrane artefakty w poszukiwaniu plików cache_shard_*.json
          shard_files = []
          for root, dirs, files in os.walk("artifacts"):
              for f in files:
                  if f.startswith("cache_shard_") and f.endswith(".json"):
                      shard_files.append(os.path.join(root, f))

          if not shard_files:
              print("No shard cache files found")
          else:
              for f in shard_files:
                  try:
                      with open(f, "r") as fp:
                          data = json.load(fp)
                  except Exception as e:
                      print(f"Error reading {f}: {e}")
                      continue

                  # Scalanie pól
                  for k in merged:
                      if k == "schema":
                          continue
                      if k not in data:
                          continue
                      if isinstance(merged[k], dict) and isinstance(data[k], dict):
                          if k == "urls_seen":
                              # Dla urls_seen wybieramy nowszy timestamp
                              for url, ts in data[k].items():
                                  if url not in merged[k] or iso_max(merged[k].get(url, ""), ts) == ts:
                                      merged[k][url] = ts
                          elif k == "dead_urls":
                              # Dla dead_urls łączymy listy bez duplikatów
                              for gkey, urls in data[k].items():
                                  if gkey not in merged[k]:
                                      merged[k][gkey] = []
                                  existing = set(merged[k][gkey])
                                  for u in urls:
                                      if u not in existing:
                                          merged[k][gkey].append(u)
                          else:
                              # Dla pozostałych słowników zwykłe update
                              merged[k].update(data[k])
                      else:
                          merged[k] = data[k]

              print(f"Merged {len(shard_files)} shard files")

          # Zapisz scalony cache.json
          os.makedirs("data", exist_ok=True)
          with open("data/cache.json", "w") as f:
              json.dump(merged, f, indent=2)

          # Spakuj cache.json do cache.zip
          with zipfile.ZipFile("data/cache.zip", "w", compression=zipfile.ZIP_DEFLATED, compresslevel=9) as z:
              z.write("data/cache.json", arcname="cache.json")

          # Opcjonalnie: usuń pliki shardowe z artifacts (nie są już potrzebne)
          # (Nie trzeba, bo artifact i tak zostanie usunięty po 30 dniach)

          print("OK: data/cache.json and data/cache.zip updated")
          PY

      - name: Commit merged cache and zip
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git pull --rebase || true
          git add data/cache.json data/cache.zip
          git commit -m "Merge shard caches [skip ci]" || echo "Nothing to commit"
          git push || true

      - name: Upload merged report (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: bip-merged-report
          retention-days: 30
          if-no-files-found: warn
          path: |
            data/cache.json
            data/cache.zip
            data/summary_report.txt
            data/log.csv
